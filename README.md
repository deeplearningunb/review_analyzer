# Review Analyzer
O Review Analyzer √© um projeto open source com o objetivo de analisar reviews. 

Utilizamos o Kibana como interface de visualiza√ß√£o, para guardar os dados usamos o Elasticsearch que √© um servidor de buscas distribu√≠do e o Logstash para fazer o processamento dos dados.

Para treinar os nossos modelos usamos as bilbioteca do Tensorflow e a do Scikit Learn.

Fique a vontade para contribuir para o reposit√≥rio com mais exemplos seja de dados ou de modelos.

## Tecnologias

<img src="https://www.tensorflow.org/images/tf_logo_social.png" alt="Tensorflow" height="100" width="200"/><img src="https://www.python.org/static/opengraph-icon-200x200.png" alt="Python" height="100" width="110"/><img src="https://i0.wp.com/kubedex.com/wp-content/uploads/2018/09/kibana-1.png" alt="Kibana" height="100" width="110"/><img src="https://miro.medium.com/max/892/1*AYP0Mg_MwJMm3Kbx8Xa8lQ.png" alt="ElasticSearch" height="100" width="200"/><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/1200px-Scikit_learn_logo_small.svg.png" alt="Scikit" height="100" width="200"/>

## Utiliza√ß√£o

### Utilizando Online
Atualmente n√£o temos um servidor com o nosso ambiente.

### Instalando Localmente
Primeiramente √© necess√°rio ter [Docker](https://docs.docker.com/install/) e o [Docker-Compose](https://docs.docker.com/compose/install/). Com eles √© poss√≠vel subir a parte de visualiza√ß√£o dos dados.

Para o Web Scrapping e subir os dados no kibana pelo Elasticsearch instale os requirements.txt

### Rodando o Ambiente

Para baixar 

```bash
cd ~/your/directory/
git clone https://github.com/deeplearningunb/review_analyzer.git
cd review_analyzer
```

Para subir os containers do Docker

```bash
docker-compose build
docker-compose up
```

Se tudo deu certo, j√° √© poss√≠vel utilizar o Kibana entrando no endere√ßo `http://localhost:5601/`. Entrando nele, voc√™ ver√° a interface do Kibana. Acessando `http://localhost:9200/`, voc√™ ter√° acesso ao Elasticsearch.

### Rodando nossas ferramentas

#### Web Scrapping
Para facilitar a analise de sentimento, n√≥s desenvolvemos algumas fun√ß√µes:

O Web Scrapping de reviews de filmes foi criado usando as bibliotecas do Beautiful Soup e do request atualmente ela s√≥ suporta o `https://www.rottentomatoes.com/`. 

Para utilizar √© necess√°rio criar um arquivo de urls.

```bash
cd web_scrapping
touch nome_do_arquivo_de_urls
vim nome_do_arquivo_de_urls
```

Dentro do arquivo escreva o nome das urls que voc√™ quer que seja feito a extra√ß√£o das reviews. Como no exemplo abaixo

```
https://www.rottentomatoes.com/m/black_panther_2018
https://www.rottentomatoes.com/m/mission_impossible_fallout
https://www.rottentomatoes.com/m/blackkklansman
https://www.rottentomatoes.com/m/spider_man_into_the_spider_verse
https://www.rottentomatoes.com/m/roma_2018
```

Depois disso basta passar o arquivo como par√¢metro.
```bash
cd web_scrapping
python3 get_review.py nome_do_arquivo_de_urls
```

#### Comunica√ß√£o Elasticsearch

Para importar os arquivos csv, √© necess√°rio fazer v√°rias requisi√ß√µes. Para agilizar esse processo, n√≥s criamos uma fun√ß√£o que faz isso para voc√™. S√≥ precisa passar os par√¢metros e ela far√° o resto.

```bash
cd import_to_kibana
python3 import_csv.py nome_do_arquivo_csv index tipo_doc
```
## Contribuidores

Agradecimentos para essas pessoas maravilhosas ([emoji key](https://allcontributors.org/docs/en/emoji-key)):
<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore -->
<table>
    <tr>
        <td align="center">
            <a href="https://github.com/cjjcastro"><img src="https://avatars0.githubusercontent.com/u/26393787?v=4" width="100px;" alt="Cleber J√∫nior" />
                <br /><sub><b>Cleber J√∫nior</b></sub></a>
            <br /><a href="https://github.com/fga-eps-mds/2019.1-hubcare-docs/commits?author=cjjcastro" title="Code">üíª</a></td>
        <td align="center">
            <a href="https://github.com/Hiroshi18"><img src="https://avatars0.githubusercontent.com/u/26282955?v=4" width="100px;" alt="Lucas Hiroshi Horinouchi" />
                <br /><sub><b>Lucas Hiroshi Horinouchi</b></sub></a>
            <br /><a href="https://github.com/fga-eps-mds/2019.1-hubcare-docs/commits?author=Hiroshi18" title="Code">üíª</a> <a href="https://github.com/fga-eps-mds/2019.1-hubcare-docs/commits?author=Hiroshi18" title="Documentation">üìñ</a></td>
        <td align="center">
            <a href="https://github.com/mateusnr"><img src="https://avatars0.githubusercontent.com/u/13491922?v=4" width="100px;" alt="Mateus N√≥brega" />
                <br /><sub><b>Mateus N√≥brega</b></sub></a>
            <br /><a href="https://github.com/fga-eps-mds/2019.1-hubcare-docs/commits?author=mateusnr" title="Code">üíª</a></td>
        <td align="center">
            <a href="https://github.com/Lorryaze"><img src="https://avatars1.githubusercontent.com/u/30262806?v=4" width="100px;" alt="Francisco Heronildo" />
                <br /><sub><b>Lorrany Azevedo</b></sub></a>
            <br /><a href="https://github.com/fga-eps-mds/2019.1-hubcare-docs/commits?author=Lorryaze" title="Code">üíª</a></td>
    </tr>
</table>

<!-- ALL-CONTRIBUTORS-LIST:END -->

## License

O nosso c√≥digo e as ferramentas desenvolvidas pela equipe est√° dentro da [MIT License](./LICENSE), mas as ferramentas externas como o Kibana e o Elastic Search s√£o Apache.
