{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_idf=pd.read_csv(\"movies_merged.csv\",sep='|')\n",
    "df_idf = df_idf.dropna()\n",
    "df_idf = df_idf.applymap(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        too few films take on the art of arguing as a ...\n",
       "1                     the film leaves a tremendous impact.\n",
       "3        from 1957 and first-time director sidney lumet...\n",
       "4        mechanically written, but within its own middl...\n",
       "5                          a strangely realistic thriller.\n",
       "6        this movie is a masterpiece. that term gets th...\n",
       "7        in the 60 years since its release, sidney lume...\n",
       "8        with each new viewing i come away feelings as ...\n",
       "9        lumet in his first film records it as a meticu...\n",
       "10       ...the film has aged surprisingly well in the ...\n",
       "11       there is real value is how it allows each memb...\n",
       "12       the film takes a confined, almost completely b...\n",
       "13       the cast is incredible, the writing superb, an...\n",
       "14       this was sidney lumet's first movie and it's a...\n",
       "15                           an incisive and gripping film\n",
       "16       the explosive qualities and historical importa...\n",
       "17       lumet keeps things tense, sweaty, suspenseful ...\n",
       "18                   outstanding henry fonda jury classic.\n",
       "19       a brilliant courtroom drama whose strength lie...\n",
       "20       12 angry men has some shrewd observations to s...\n",
       "21                   taut, well-crafted sociological study\n",
       "22           not much action, lots of words, great cinema.\n",
       "23       every bit the classic it's been made out to be...\n",
       "24       power house jury room drama w/cast of memorabl...\n",
       "25       sidney lumet... effectively modulates the dram...\n",
       "27       makes no self-saluting motions to greater impo...\n",
       "28       sidney lumet's oscar-nominated feature directi...\n",
       "29       a masterful work of debate and dialogue; of sh...\n",
       "31       this film is a great combination of an adult t...\n",
       "32       ainda que se passe em um nico ambiente, esta o...\n",
       "                               ...                        \n",
       "62504    if zootopia only reluctantly comes around to i...\n",
       "62505    zootopia proves a successful amalgamation of d...\n",
       "62506    heavy with pop allusions and references to oth...\n",
       "62507    zootopia's message of tolerance is a noble one...\n",
       "62508    \"a very clever movie with the sort of meticulo...\n",
       "62509    delightful disney animated tale of a city popu...\n",
       "62510    a neo-noir buddy cop comedy that will likely e...\n",
       "62511    it has the quality seal of all disney films, b...\n",
       "62512    it's deffinately not this year's greates anima...\n",
       "62513    the world that zootropolis creates is intellig...\n",
       "62514    smart, but better at delivering a message of t...\n",
       "62515    the most ingenious part of the film is buildin...\n",
       "62516    zootopia isn't simply another fun disney anima...\n",
       "62517    a solid job with its own message that can be e...\n",
       "62518    disney created a fun fable, as smart as a dolp...\n",
       "62519    zootopia makes a statement about how we should...\n",
       "62520    zootopia has all the elements to become a clas...\n",
       "62521    this is probably one of the best films disney ...\n",
       "62522    it's funny, it's clever, it's exciting and sus...\n",
       "62523    a wild ride with some noir-ish undertones, zoo...\n",
       "62524    a smart play on \"the race card\" in this fun an...\n",
       "62525    disney creates a new classic, let's just hope ...\n",
       "62526    zootopia continues the wining streak disney ha...\n",
       "62527    the be-true-to-yourself and don't-judge-others...\n",
       "62528    a noir comedy that carries itself with loads o...\n",
       "62529    the variety of cute - and occasionally slightl...\n",
       "62530    zootopia is a big surprise that offers quality...\n",
       "62531    just when it was looking like animated animal ...\n",
       "62532    disney offers a decades-later correction to 's...\n",
       "62533    with its upfront themes of prejudice and diver...\n",
       "Name: Text, Length: 62008, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiroshi/Documents/env/gpam-stf/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"stopwords.txt\")\n",
    "\n",
    "#get the text column \n",
    "docs=df_idf['Text'].tolist()\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62008, 35025)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62008, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films',\n",
       " 'take',\n",
       " 'art',\n",
       " 'arguing',\n",
       " 'subject',\n",
       " 'certainly',\n",
       " 'lumet',\n",
       " 'window',\n",
       " 'strained',\n",
       " 'civic']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the vocabulary by using `get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['creaky',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creates',\n",
       " 'creating',\n",
       " 'creation',\n",
       " 'creations',\n",
       " 'creative',\n",
       " 'creatively',\n",
       " 'creativity',\n",
       " 'creator',\n",
       " 'creators',\n",
       " 'creature',\n",
       " 'creatures']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.get_feature_names())[2000:2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the IDF values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.32698461, 6.89923638, 8.02770163, ..., 7.410062  , 8.56929891,\n",
       "       8.0837911 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test docs into a list\n",
    "docs_test=df_idf['Text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing TF-IDF and Extracting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Keywords===\n",
      "civic 0.332\n",
      "arguing 0.329\n",
      "mightily 0.318\n",
      "strained 0.314\n",
      "lumet 0.314\n",
      "duty 0.304\n",
      "window 0.276\n",
      "serve 0.275\n",
      "continue 0.254\n",
      "certainly 0.206\n"
     ]
    }
   ],
   "source": [
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs_test[0]\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the common code into several methods\n",
    "def get_keywords(idx):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords):\n",
    "    # now print the results\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at keywords generated for a much longer question: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Keywords===\n",
      "mankiewicz 0.409\n",
      "joseph 0.393\n",
      "eve 0.384\n",
      "qualities 0.348\n",
      "positive 0.341\n",
      "list 0.323\n",
      "amazing 0.276\n",
      "top 0.26\n",
      "performances 0.213\n"
     ]
    }
   ],
   "source": [
    "idx=120\n",
    "keywords=get_keywords(idx)\n",
    "print_results(idx,keywords)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
